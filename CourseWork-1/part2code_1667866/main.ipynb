{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import*\n",
    "from helpers import PopulateCsvFile, RequestValue, CreateDir\n",
    "from MLHelpers import InitDataFrame, SplitDataFrame, ExcelAndShowCase\n",
    "from Training import TrainModels\n",
    "from Evaluation import EvaluateModels, EvaluateString\n",
    "from TrainingCVNonOptimized import TrainModelsOnCvNonOptimized\n",
    "from TrainingCVOptimized import TrainModelsCVOptimized\n",
    "CreateDir(\"Results\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#init csv file with all raw data and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PopulateCsvFile(RequestValue(variablesDict, \"rawDataPath\"), RequestValue(variablesDict, \"targetCsvFile\"), \n",
    "                RequestValue(variablesDict, \"fieldNames\") , RequestValue(variablesDict, \"encodingType\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#init dataframe, split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inits dataframe\n",
    "mainDataFrame = InitDataFrame(RequestValue(variablesDict, \"targetCsvFile\"), \n",
    "                              RequestValue(variablesDict, \"encodingType\") )\n",
    "\n",
    "\n",
    "    #split training and validation\n",
    "xTrain, xValid, yTrain, yValid = SplitDataFrame(mainDataFrame, \n",
    "                                                RequestValue(variablesDict, \"testSize\"), RequestValue(variablesDict, \"randomSeed\"))\n",
    "\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nxTrain len: \",len(xTrain), \"\\nxValid len: \",len(xValid), \"\\nyTrain len: \", len(yTrain), \"\\nyValid len: \" , len(yValid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train models on features without cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments = (modelsDict, featuresDict, xTrain, yTrain,kFeatures)\n",
    "TrainModels(modeslDictionary, nonOpFeaturesDictionary, xTrain, yTrain, RequestValue(variablesDict, \"kFeatures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluate models against validation/test set returns data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalEvaluationScores = EvaluateModels(xValid, yValid, \"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the following is dataframe of the evaluation results from multiple models against the defult settings of multiple features(see constants.py for further details on models and features)\n",
    "#this operation is without cross validation and directly fits a pipeline wtih each model \n",
    "#the dataframe is dumped into an excel sheet in cwd/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(normalEvaluationScores, \"/normalEvaluationScores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train non optimized cross-validation and returns a dataframe of training scores and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonOpTrainScores, nonOpTestScores = TrainModelsOnCvNonOptimized(modeslDictionary,nonOpFeaturesDictionary, mainDataFrame, RequestValue(variablesDict, \"kFeatures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the following is dataframe of the train and test results from multiple models against the defult settings of multiple features(see constants.py for further details on models and features) with non-optimized cross validation utilizing cross_validate from sklearn\n",
    "#mean of train score for all folds for each model/clf and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(nonOpTrainScores, \"/nonOpTrainScores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mean of test score for all folds for each model/clf and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(nonOpTestScores, \"/nonOpTestScores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train cv optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opTrainScores, opTestScores = TrainModelsCVOptimized(modeslDictionary, featuresDictionary, xTrain, yTrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the following is dataframe of the test and train results from multiple models and features optimized with cross-validation and hyper param tuning utilizing graphsearchcv\n",
    "#mean of train score for all folds for each model/clf and feature using the best parms (from graph search cv from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(opTrainScores, \"/opTrainScores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mean of test score for all folds for each model/clf and feature using the best parms (from graph search cv from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(opTestScores, \"/opTestScores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#evaluation scores from xvalid and yvalid against the best models from the CVGraphsearch results (optimized cross-validaited models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opEvalScores = EvaluateModels(xValid, yValid, \"optimized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data evaluation scores from xvalid and yvalid against the best models from the CVGraphsearch results (optimized cross-validaited models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcelAndShowCase(opEvalScores, \"/opEvalScores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal-LogisticRegression-CountVectorizer.joblib  predicted: ['sport']\n",
      "Normal-LogisticRegression-TFDIFChar.joblib  predicted: ['sport']\n",
      "Normal-LogisticRegression-TFDIFUnigram.joblib  predicted: ['sport']\n",
      "Normal-NaiveBayes-CountVectorizer.joblib  predicted: ['politics']\n",
      "Normal-NaiveBayes-TFDIFChar.joblib  predicted: ['sport']\n",
      "Normal-NaiveBayes-TFDIFUnigram.joblib  predicted: ['sport']\n",
      "Normal-SVM-CountVectorizer.joblib  predicted: ['sport']\n",
      "Normal-SVM-TFDIFChar.joblib  predicted: ['sport']\n",
      "Normal-SVM-TFDIFUnigram.joblib  predicted: ['sport']\n"
     ]
    }
   ],
   "source": [
    "EvaluateString('IAAF launches fight against drugsThe IAAF - athletics world governing body - has met anti-doping officials, coaches and athletes to co-ordinate the fight against drugs in sport.Two task forces have been set up to examine doping and nutrition issues. It was also agreed that a programme to de-mystify the issue to athletes, the public and the media was a priority. Nothing was decided to change things - it was more to have a forum of the stakeholders allowing them to express themselves, said an IAAF spokesman. Getting everyone together gave us a lot of food for thought. About 60 people attended Sundays meeting in Monaco, including IAAF chief Lamine Diack and Namibian athlete Frankie Fredericks, now a member of the Athletes Commission. I am very happy to see you all, members of the athletics family, respond positively to the IAAF call to sit together and discuss what more we can do in the fight against doping, said Diack. We are the leading Federation in this field and it is our duty to keep our sport clean. The two task forces will report back to the IAAF Council, at its April meeting in Qatar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a895bd21b1a1d2731361ff59fc708282165d760aa0414dacdfeb3d2c9f175cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
